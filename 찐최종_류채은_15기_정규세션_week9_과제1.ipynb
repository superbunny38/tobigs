{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "찐최종_류채은_15기_정규세션_week9_과제1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superbunny38/tobigs/blob/master/%EC%B0%90%EC%B5%9C%EC%A2%85_%EB%A5%98%EC%B1%84%EC%9D%80_15%EA%B8%B0_%EC%A0%95%EA%B7%9C%EC%84%B8%EC%85%98_week9_%EA%B3%BC%EC%A0%9C1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytaWF9tLFQ0T"
      },
      "source": [
        "# Week 9 과제 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btozx60gSkCX"
      },
      "source": [
        "**잎 사진을 통한 질병 분류**\n",
        "\n",
        "*   데이터: 21개의 클래스로 구분된 약 20000장의 이미지\n",
        "><img src=\"https://drive.google.com/uc?id=1YQkxnNy61Gyi3Gp6ylCKeS72BVruJXr_\" width=\"700\" height=\"500\"> \n",
        "*   데이터 전처리\n",
        "    *   전체 데이터를 train,validation,test로 분할해주세요\n",
        "    *   저는 6:2:2로 분할했는데, 비율은 바꾸셔도 무방합니다.\n",
        "   \n",
        "    \n",
        "*   학습 진행 방향\n",
        "    *   Baseline 모델(pre-trained model 사용X) 구축\n",
        "    *   Pre-trained 모델 사용\n",
        "    *   Baseline과 ResNet50 모델의 성능을 비교해주세요 ~!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSXsUPLeJYmh",
        "outputId": "d2243d55-2716-47a7-9a89-161eeb51a4e2"
      },
      "source": [
        "# gdrive에 mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# 경로 설정\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IchWiK1ZFQ0X"
      },
      "source": [
        "## 0. 데이터 분할"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jEtmXkaFQ0Z"
      },
      "source": [
        "* 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fG-dk2dFQ0Z"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        " \n",
        "original_dataset_dir = '/content/gdrive/MyDrive/Colab Notebooks/week9_모델심화1/tobigs_week9_plant_leaf_data/tobigs_week9_plant_leaf'   \n",
        "classes_list = os.listdir(original_dataset_dir) \n",
        " \n",
        "base_dir = '/content/gdrive/MyDrive/Colab Notebooks/week9_모델심화1/splitted' \n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') \n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list:     \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oQyurQ6FQ0a"
      },
      "source": [
        "* 데이터 분할과 클래스별 데이터 수 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nrkw0nFQ0b"
      },
      "source": [
        "import math\n",
        " \n",
        "for cls in classes_list:\n",
        "    path = os.path.join(original_dataset_dir, cls)\n",
        "    fnames = os.listdir(path)\n",
        " \n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    train_fnames = fnames[:train_size]\n",
        "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
        "\n",
        "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZDnksn0FQ0c"
      },
      "source": [
        "## 1. 베이스라인 모델을 구축해 주세요\n",
        "* Pre-Trained Model을 사용하지 않고 직접 모델을 구축해 주세요 !\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm7kPbfsFQ0d"
      },
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "BATCH_SIZE = 256 \n",
        "EPOCH = 30 "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl_j5EzoFQ0d"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder \n",
        " \n",
        "transform_base = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()]) \n",
        "train_dataset = ImageFolder(root='/content/gdrive/MyDrive/Colab Notebooks/week9_모델심화1/splitted/train', transform=transform_base) \n",
        "val_dataset = ImageFolder(root='/content/gdrive/MyDrive/Colab Notebooks/week9_모델심화1/splitted/val', transform=transform_base)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djFeShktFQ0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "610b733a-e857-434d-bbdc-69bc35e9f9d9"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDEhzqZRFQ0e"
      },
      "source": [
        "* 베이스라인 모델 설계하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJE-jG9UFQ0e"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        " \n",
        "class Net(nn.Module): \n",
        "  \n",
        "    def __init__(self): \n",
        "    \n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding = 1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3,padding = 1)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(256*2*2, 50)#21은?\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "        ''' \n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv2d(3, 64, kernel_size=3,padding=1),\n",
        "            BatchNorm2d(64,eps=1e-05, momentum=0.1, affine=True),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer\n",
        "            Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(256, 21)\n",
        "        )'''\n",
        "        #\"코드 작성해주세요\"\n",
        "    \n",
        "    def forward(self, x):  \n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 256*2*2)#980..   \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model_base = Net().to(DEVICE)  \n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LowbXUGKFQ0e"
      },
      "source": [
        "* 모델 학습을 위한 함수\n",
        "* 모델 학습, 평가를 위한 가이드라인 코드입니다. 꼭 이 코드를 사용하지는 않으셔도 됩니다 !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GL1mNqzFQ0f"
      },
      "source": [
        "from torchvision import models\n",
        " \n",
        "def train(model, train_loader, optimizer):\n",
        "    model.train() \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE) \n",
        "        optimizer.zero_grad() \n",
        "        output = model(data)  \n",
        "        loss = nn.CrossEntropyLoss()   #Cross Entropy Loss 사용했습니다\n",
        "        loss.backward()\n",
        "        optimizer.step() "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqvOveaxFQ0f"
      },
      "source": [
        "* 모델 평가를 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncm8J6v4FQ0f"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.evaluate()\n",
        "    test_loss = 0 \n",
        "    correct = 0   \n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)  \n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.cross_entropy(output,target, reduction='sum').item() \n",
        " \n",
        "            \n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) \n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
        "    return test_loss, test_accuracy  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us-wigmEFQ0g"
      },
      "source": [
        "* 모델 학습을 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_hvkStsFQ0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "578f3588-233e-49f3-d033-b26cb01ce9df"
      },
      "source": [
        "import time\n",
        "import copy\n",
        " \n",
        "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0  \n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
        " \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        since = time.time()  \n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader)\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        \n",
        "        if val_acc > best_acc: \n",
        "            best_acc = val_acc \n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = time.time() - since \n",
        "        print('-------------- epoch {} ----------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
        "\n",
        "    model.load_state_dict(best_model_wts)  \n",
        "    return model\n",
        " \n",
        "\n",
        "base = train_baseline(model_base, train_loader, val_loader, optimizer)  \t\n",
        "torch.save(base,'baseline.pt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f513f4b77ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'baseline.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f513f4b77ace>\u001b[0m in \u001b[0;36mtrain_baseline\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msince\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-92835cfbaea8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#Cross Entropy Loss 사용했습니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 948\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CrossEntropyLoss' object has no attribute 'backward'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mV-CQwZFQ0h"
      },
      "source": [
        "## 2. Transfer Learning 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDoeutltFQ0h"
      },
      "source": [
        "* Transfer Learning을 위한 준비\n",
        "* Transfer Learning이 익숙하지 않으신 분들은 PyTorch에서 제공하는 https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/transfer_learning_tutorial.html 을 참고하세요 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so4pOWIqFQ0i"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), \n",
        "        #'이 부분에   #Data Augmentation도 진행해주세요 !!'\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]),\n",
        "    \n",
        "    'val': transforms.Compose([transforms.Resize([64,64]),  \n",
        "        #'validation 데이터는 Test 데이터와 동일한 조건이어야 합니다.'\n",
        "        #'validation에 맞는 전처리를 수행해 주세요',\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Tbg8YzFQ0j"
      },
      "source": [
        "### Pre-Trained Model 불러오기\n",
        "- 저는 ResNet50을 사용했는데, 코랩 기준으로 다른 ResNet계열이나 DenseNet 정도까지는 큰 무리 없이 훈련할 수 있습니다. Unfreeze layer 수가 적으면 다른 모델도 사용할 수 있을 것입니다.\n",
        "- 한 가지 모델을 선택해서 Transfer Learning을 해 주세요 !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3CwSwMWFQ0j"
      },
      "source": [
        "from torchvision import models\n",
        " \n",
        "resnet = models.resnet50(pretrained=True)  \n",
        "num_ftrs = resnet.fc.in_features   \n",
        "#resnet.fc = nn.Linear(\"코드 작성해주세요\", \"코드 작성해주세요\") \n",
        "resnet.fc = nn.Linear(256, 21) \n",
        "resnet = resnet.to(DEVICE)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
        " \n",
        "from torch.optim import lr_scheduler  #scheduler는 사용하지 않으셔도 됩니다 (선택사항)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)  #7에폭마다 learning rate를 조절하는 역할을 합니다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQoS058cFQ0k"
      },
      "source": [
        "* Pre-Trained Model의 일부 Layer Freeze하기 (resnet 기준입니다 !!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXzgnWxkFQ0k"
      },
      "source": [
        "ct = 0 \n",
        "for child in resnet.children():  \n",
        "    ct+= 1  \n",
        "    if ct < 6: \n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWdXTYBaFQ0l"
      },
      "source": [
        "* Fine Tuning을 진행해주세요 ~!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZXgc0G_FQ0m"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q42vR5_FQ0n"
      },
      "source": [
        "* 모델 평가를 위해서는 평가 데이터 또한 전처리를 해주어야 합니다.\n",
        "* validation 데이터와 동일하게 전처리를 해 주세요 ~!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr5LkMzMRxVk"
      },
      "source": [
        "* 베이스라인 모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EfLf3GOFQ0n"
      },
      "source": [
        "transform_base = transforms.Compose([[Rescale(256),\n",
        "                               RandomCrop(224)]])\n",
        "test_base = ImageFolder(root='./splitted/test',transform=transform_base)  \n",
        "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KISksPR3FQ0n"
      },
      "source": [
        "* Transfer Learning모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BTGBjTzFQ0n"
      },
      "source": [
        "transform_resNet = transforms.Compose([\n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "    ])\n",
        "    \n",
        "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet) \n",
        "test_loader_resNet = torch.utils.data.DataLoader(test_resNet,batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J4OJRfCSrrM"
      },
      "source": [
        "### 성능 평가하기\n",
        "* 저는 여기서 accuracy만을 평가했지만, 분류 모델이기에 다양한 방법의 평가가 가능합니다.\n",
        "* Confusion Matrix를 이용한 비교도 가능하고, 한 작물에 해당하는 클래스가 여러개인 다중 분류에서 F1-score를 계산하는것도 의미가 있을 것입니다. \n",
        "* 다양한 시도를 하시는 분께 가산점 드리겠습니다 :):)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGFNIsYfFQ0o"
      },
      "source": [
        "* 베이스라인 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsJCTvboFQ0o"
      },
      "source": [
        "baseline=torch.load('baseline.pt') \n",
        "baseline.eval()  \n",
        "test_loss, test_accuracy = evaluate(baseline, test_loader)\n",
        "\n",
        "print('test acc:  ', test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guqq1ZHiFQ0o"
      },
      "source": [
        "* Transfer Learning 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxfqIEkeFQ0o"
      },
      "source": [
        "resnet50=torch.load('resnet50.pt') \n",
        "resnet50.eval()  \n",
        "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
        "\n",
        "print('test acc:  ', test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8wZSpB6R7c1"
      },
      "source": [
        "* 두 모델의 성능을 비교 평가하는 설명을 작성해주세요 ~!!"
      ]
    }
  ]
}